\documentclass[10pt,aspectratio=169]{beamer}
\usetheme{default}
\setbeamercovered{invisible}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}{
    \flushright{\hfill \insertframenumber{}/\inserttotalframenumber}
}

\usepackage{listings}
\lstdefinestyle{cpp}{
	backgroundcolor = \color{blue!5!white},
	aboveskip=10pt, belowskip=10pt,
	numbers=left, numberstyle=\tiny,
	tabsize=2,
	breaklines=true,
	basicstyle=\scriptsize\ttfamily,
	keywordstyle = \color{blue!50!cyan}\bfseries,
	commentstyle = \color{darkgray!90},
	stringstyle = \color{OliveGreen},
	morecomment=[l][\color{violet!90!black}]{\#},
	identifierstyle=\color{blue!25!black}
}

\begin{document}
    \title{Standard Algorithms}
\author{Paolo Joseph Baioni\\ \href{mailto:paolojoseph.baioni@polimi.it}{\color{blue}paolojoseph.baioni@polimi.it}}
\date{\today}
    
\begin{frame}[plain, noframenumbering]
    \maketitle
\end{frame}

\begin{frame}{Recap I - Horner algorithm}
	Evaluating high order polynomial can be computationally expensive in terms of floating point operations. \\
	
	\medskip
	
Given a polynomial $\displaystyle p(x) = \sum_{i=0}^{n} a_i x^i$, instead of directly evaluating all the monomials at $x_0$, we compute the following sequence:
\begin{equation*} 
\begin{aligned}
	b_n &= a_n, \\
	b_{n-1} &= a_{n-1} + b_n x_0, \\
	\vdots \\
	b_{0} &= a_{0} + b_1 x_0.
\end{aligned}
\end{equation*}

	The Horner rule exploits the already computed informations to reduce the number of floating point operations.\\
	
	\medskip
	
	\textbf{Curiosity:} \textit{ Horner's rule is optimal when evaluating scalar polynomials sequentially.}
\end{frame}


\begin{frame}{Recap II - Horner algorithm}
		
\begin{enumerate}
    \item Implement the \texttt{eval()} and \texttt{eval\_horner()} functions to compute:
    \begin{align*}
        p_\text{eval}(x) &= a_0 + a_1x + a_2x^2 + a_3x^3 + \ldots + a_nx^n, \\
        p_\text{Horner}(x) &= a_0 + x \left(a_1 + x \left(a_2 + x \left(a_3 + \ldots + x\left(a_{n-1} + x \, a_n\right) \ldots \right) \right) \right).
    \end{align*}
    \item Implement an \texttt{evaluate\_poly()} function by manually looping over the input points.
    \item $\rightarrow$ Modify \texttt{evaluate\_poly()} to use \textbf{ std::transform}.
    \item $\rightarrow$ Implement an \texttt{evaluate\_poly\_parallel()} that makes use of the \textbf{parallel execution policies} of \texttt{std::transform} (available since \texttt{C++17}).
    \item Convert \texttt{eval} and \texttt{eval\_horner} from function pointers to \texttt{std::function}.
    \item (\textit{Homework}) Let the user choose from a json parameters file the degree of the polynomial and the discretization interval. The \texttt{json} library can be installed by running \texttt{./install\_PACS.sh} from \texttt{\$\{PACS\_ROOT\}/Extras/json}
\end{enumerate}

\textbf{NB}: the parallel version requires to link against the Intel Threading Building Blocks (\texttt{TBB}) library (preprocessor flags \texttt{-I\$\{mkTbbInc\}}, linker flags \texttt{-L\$\{mkTbbLib\} -ltbb}).
\end{frame}

\begin{frame}{A dive into std::algorithms \& execution policies}
Ingredients:
    \begin{itemize}
        \item lambdas \& function objects
        \item C++17 \href{https://en.cppreference.com/w/cpp/algorithm}{\color{blue}standard algorithms} and \href{https://en.cppreference.com/w/cpp/algorithm/execution_policy_tag_t}{\color{blue} execution policies}
        \item gcc toolchain and tbb module (or equivalent in the course docker/podman container)
        \item extra: ad-hoc compilers for offloading execution to devices (eg GPU, but works also on CPU), such as \href{https://catalog.ngc.nvidia.com/orgs/nvidia/containers/nvhpc}{\color{blue}nvhpc container}*.
    \end{itemize}
    \vfill
    {\footnotesize *Can be built with:\\
    {\ttfamily sudo docker run --gpus all -it --rm nvcr.io/nvidia/nvhpc:25.1-devel-cuda\_multi-ubuntu24.04} \\
    or \\
    {\ttfamily singularity build nvhpc.sif docker://nvcr.io/nvidia/nvhpc:25.1-devel-cuda\_multi-ubuntu24.04}\\
    Requires proper drivers for GPU execution.\\
    It takes some minutes to download, so, if you want to use it, it might be better to start the build now}
\end{frame}

\begin{frame}[fragile]{Lambdas and Functors recap}
    \begin{enumerate}
        \item Lambdas are un-named function objects
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
vector<real_t> v = {0, 1, 2, 3};
real_t a = 4.;
auto f = [&v,a](idx_t i) { return v[i] * a; };
cout << "4 = " << f(1) << endl;

struct __unnamed 
{
 real_t s;
 vector <real_t>& v;
 real_t operator()(int i) { return v[i] * a;}
};
__unnamed f{a, v};
cout << "4 = " << f(1) << endl;
\end{lstlisting}
\item You can pass vectors also copying pointers (gpu safe, see last slides)
        \begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
vector<real_t> v = {0, 1, 2, 3};
real_t a = 4.;
auto f = [v = v.data (), a](idx_t i) { return v[i] * a; };
cout << "4 = " << f(1) << endl;
\end{lstlisting}
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{C++17 Algorithms Recap}
From for loops
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
vector <real_t> u (3), v = {0, 1, 2};
for (idx_t i = 0; i < u.size () ; ++i)
  u[i] =  v[i];
\end{lstlisting}
to std::algorithm
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
transform (v.cbegin (), v.cend (), u.begin (), [](const real_t & vi){return vi;});
\end{lstlisting}
That takes an optional {\ttfamily ExecutionPolicy\&\& policy} as first parameter
\end{frame}

\begin{frame}[fragile]{C++17 - C++20 Execution Policies - I}
The execution policy types:
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
std::execution::sequenced_policy
std::execution::parallel_policy
std::execution::parallel_unsequenced_policy
std::execution::unsequenced_policy
\end{lstlisting}
have the following respective instances:
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
std::execution::seq,
std::execution::par,
std::execution::par_unseq, and
std::execution::unseq.
\end{lstlisting}
These instances are used to specify the execution policy of parallel algorithms, i.e., the kinds of parallelism allowed.
\end{frame}
\begin{frame}{C++17 - C++20 Execution Policies - II}
\begin{enumerate}
    \item \textbf{seq}: requires that a parallel algorithm's execution may not be parallelized.
    \item \textbf{par}: indicates that a parallel algorithm's execution may be parallelized. The invocations of element access functions in parallel algorithms invoked with this policy are permitted to execute in either the invoking thread or in a thread implicitly created by the library to support parallel algorithm execution.
    \item \textbf{par\_unseq}: indicates that a parallel algorithm's execution may be parallelized, vectorized, or migrated across threads. The invocations of element access functions in parallel algorithms invoked with this policy are permitted to execute in an unordered fashion in unspecified threads, and unsequenced with respect to one another within each thread.
    \item \textbf{unseq} (C++20): indicates that a parallel algorithm's execution may be vectorized, e.g., executed on a single thread using instructions that operate on multiple data items.
\end{enumerate}
\vfill
\textbf{Remark} When using any parallel execution policy, it is the programmer's responsibility to avoid data races and deadlocks.
\end{frame}

\begin{frame}[fragile]{Indexing}
    In for loops we can use plain indices; given some functional {\ttfamily f} and vector v, we have
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
for (idx_t i = 0; i < v.size (); ++i) 
    v[i] = f(i);
\end{lstlisting}
 

    \bigskip
    In std::algorithms we can use:
    \begin{itemize}
        \item raw pointers (error prone, not suggested)
        \item index vectors (memory inefficient, not suggested)
        \item counting iterators (external library required)
        \item C++20 std::ranges and std::views (full support from C++23, eg, for n-dimensional arrays)
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Raw pointers}
Compute index as difference between each element memory address and raw pointer to vector data 

\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
std::transform (v.begin (), v.end (), 
               [v = v.data(), f](real_t vi) 
               {
                 auto i = &vi - v;
                 return f(i);
               });
\end{lstlisting}

\end{frame}

\begin{frame}[fragile]{Vectors of indices}
Easiest solution, highly inefficient

\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
std::vector <idx_t> indices (v.size ());
std::iota (indices.begin (), indices.end (), 0); 
// now indices is {0,1,2...v.size()-1}
std::for_each (indices.begin (), indices.end (), 
               [v = v.data()](idx_t i) 
               {
                 v[i] = f(i);
               });
\end{lstlisting}
Almost equal to the traditional for loop: ease of porting, but we are allocating a full vector of indices

\textbf{Remark} Avoid using vector of indices, it is shown here only for didactic purposes, to help giving an operational definition of counting iterators and views in this realm.
\end{frame}

\begin{frame}[fragile]{Counting iterators}
One of the best available solutions, requires:
\begin{itemize}
    \item C++17
    \item a library (boost and thrust are header only, it is enough to include them; boost is in the mk modules)
\end{itemize}

\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
boost::counting_iterator <idx_t> first(0), last(v.size ());
std::for_each (first, last, 
               [v = v.data(), f](idx_t i) 
               {
                 v[i] = f(i);
               });
\end{lstlisting}
\textbf{Remark:} compile with {\ttfamily g++ -std=c++20 -I\$mkBoostInc boost\_example.cpp}
\end{frame}

\begin{frame}[fragile]{Ranges and views}
One of the best available solutions; features:
\begin{itemize}
    \item C++20 required, 23 for full support
    \item no external dependencies needed
\end{itemize}
C++20:
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
auto indices = std::views::iota (0, v.size ());
std::for_each (indices.begin (), indices.end (),
              [v = v.data()](idx_t i) 
              {
                v[i] = f(i);
              });
\end{lstlisting}
From C++23* 
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
std::views::cartesian_product(std::views::iota (0, m), std::views::iota (0, n));
...
\end{lstlisting}
\textbf{Remark:} remember to {\ttfamily \#include <ranges>} for the definition of {\ttfamily std::views::iota}\\\bigskip
\vfill
{\small  *In previous standards you could rely on proposals for missing features, eg\\ \href{https://github.com/ericniebler/range-v3}{\color{blue}https://github.com/ericniebler/range-v3}, but, in case, it becomes competitive with boost}
\end{frame}

\begin{frame}{Example application: Single/Double precision A $\times$ X + Y}
SAXPY and DAXPY stands for Single and Double-Precision A $\times$ X + Y\\
It is a basic operation in linear algebra and is commonly used in numerical computing.\\
We will use it to show examples of:
\begin{itemize}
    \item basic std algoritghms usage
    \item efficient std algorithms usage
    \item parallel std algorithms usage and performance metrics (bandwidth)
    \item possible effects of floats precision
\end{itemize}
\end{frame}


\begin{frame}[fragile]{Saxpy - slow}
Using STL defined std::algorithms only
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
void saxpy_slow (float a, std::vector <float> & x, std::vector <float> & y)
{
  std::vector <float> temp (x.size ());
  std::fill (temp.begin (), temp.end (), a);
  std::transform (x.begin (), x.end (), temp.begin (), temp.begin (), std::multiplies <float> ());
  std::transform (temp.begin (), temp.end (), y.begin (), y.begin (), thrust::plus <float> ());
}
\end{lstlisting}

\end{frame}

\begin{frame}[fragile,allowframebreaks]{Saxpy - fast - full example}
Using user-defined function object and {\ttfamily std::for\_each}
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
#include <algorithm>
#include <iostream>
#include <vector>
#ifdef USE_BOOST
  #include <boost/iterator/counting_iterator.hpp>
#else
  #include <ranges>
#endif

using idx_t = size_t;
using real_t = float;

int main()
{
  std::vector <real_t> x{0,1,2,3,4}, y{0,1,2,3,4};
  real_t a (5.);
  
  auto saxpy = [x = x.data (), y = y.data (), a] 
               (idx_t i)
	             { y[i] += a * x[i];};

  
  #ifdef USE_BOOST
		boost::counting_iterator <idx_t> first(0), last(x.size ());
		std::for_each (first, last, saxpy);
  #else
  	auto indices = std::views::iota (static_cast<std::vector<real_t>::size_type> (0),  x.size ());
	  std::for_each (indices.begin (), indices.end (), saxpy);           
  #endif
  
  for (idx_t i=0; i<x.size(); ++i)
  	std::cout << 5 << "x" << x[i] << "+" << x[i] << "=" << y[i] << std::endl;
  return 0;
}
\end{lstlisting}
Compile with {\ttfamily g++ -std=c++20 -Wall saxpy.cpp -o views} , or: 
\begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
source /u/sw/etc/bash.bashrc
module load gcc-glibc
module load boost
g++ -std=c++20 -Wall -I$mkBoostInc -DUSE_BOOST saxpy.cpp -o boost
\end{lstlisting}
and test with {\ttfamily ./views} or {\ttfamily ./boost}
\end{frame}

\begin{frame}[fragile]{Notes on parallel execution - I}
\begin{itemize}
    \item you can modify the above saxpy.cpp to run in parallel, adding {\ttfamily \#include <execution>} and {\ttfamily std::execution::par\_unseq} in the {\ttfamily for\_each}
    \item if you use g++ (or clang++), you have to link against a parallel library, such as tbb. In our case, eg with boost, besides the above steps, we have:
    \begin{lstlisting}[frame=single, style=cpp, firstnumber=1, numbers=left, numberstyle=\tiny,showtabs=false,xleftmargin=.05\linewidth,xrightmargin=.025\linewidth]
module load tbb
g++ -std=c++20 -Wall -I$mkBoostInc -I$mkTbbInc -L$mkTbbLib -ltbb -DUSE_BOOST saxpy.cpp -o boost+tbb
./boost+tbb
\end{lstlisting}
\item if you have nvhpc sdk (software development toolkit), you can compile also with
\begin{itemize}
    \item cpu: {\ttfamily nvc++ -stdpar=multicore -std=c++20 -O4 -fast -march=native -Mllvm-fast -DNDEBUG -o saxpy saxpy.cpp}
    \item NVIDIA gpu: {\ttfamily nvc++ -stdpar=gpu -std=c++20 -O4 -fast -march=native -Mllvm-fast -DNDEBUG -o saxpy saxpy.cpp}
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Notes on parallel execution - II}
\begin{itemize}
\item mind that GPUs are not CPUs with more threads; the hardware solutions introduced to achieve their parallelism and memory bandwidth requires constraints on the code, which currently, for nvc++, can be translated in the following recommendations:
\begin{itemize}
    \item prefer declaring and defining functions to be run on device in the same file (eg, define a full functor in a .hpp)
    \item prefer dynamically allocated containers, such as std::vectors instead of std::arrays
    \item avoid function pointers
    \item avoid references in lambda captures
    \item avoid exeptions
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Notes on parallel execution - III}
\begin{itemize}
\item for performance metrics, check the uploaded ``bandwidth.cpp" example. You can compile with
\begin{itemize}
    \item {\ttfamily g++ -std=c++20 -Wall -Ofast -march=native -DNDEBUG -o daxpy\_g++\_tbb bandwidth.cpp -ltbb}
    \item {\ttfamily clang++ -std=c++20 -Wall -O3 -ffast-math -march=native -DNDEBUG -o daxpy\_clang++\_tbb bandwidth.cpp -ltbb}
    \item {\ttfamily nvc++ -stdpar=multicore -std=c++20 -Wall -O4 -fast -march=native -Mllvm-fast -DNDEBUG -o daxpy\_nvc++\_multicore bandwidth.cpp}
    \item {\ttfamily nvc++ -stdpar=gpu -std=c++20 -Wall -O4 -fast -march=native -Mllvm-fast -DNDEBUG -o daxpy\_nvc++\_gpu bandwidth.cpp}
\end{itemize}
\item and compare the results of, eg
\begin{itemize}
    \item {\ttfamily ./daxpy\_g++\_tbb 1000000} and {\ttfamily ./daxpy\_nvc++\_gpu 1000000}
    \item {\ttfamily ./daxpy\_g++\_tbb 100000000} and {\ttfamily ./daxpy\_nvc++\_gpu 100000000}
\end{itemize}
for {\ttfamily real\_t=float} and {\ttfamily real\_t=double}
\end{itemize}
    
\end{frame}

\begin{frame}{Exercise (homework)}
    Return to our Horner - std::transform solution, and try to apply the new material.
\end{frame}

\begin{frame}{References}
\begin{itemize}
    \item \href{https://en.cppreference.com/w/cpp/}{https://en.cppreference.com/}
    \item \href{https://developer.nvidia.com/}{https://developer.nvidia.com/}
    \item \href{https://www.boost.org/}{https://www.boost.org/}
    \item \href{https://github.com/ericniebler}{https://github.com/ericniebler}
\end{itemize}
    
\end{frame}

\end{document}
